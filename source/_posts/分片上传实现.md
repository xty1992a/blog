---
title: 分片上传实现
tags:
  - 浏览器
  - nodejs
categories:
  - - 技术
    - 浏览器
abbrlink: 162c0a77
date: 2020-05-25 17:16:59
---

### 简述
作为前端,nodjs还是要有所了解的.  
偶尔练练手也是有必要的.  
最近看了一些文章,关于大规格文件上传的问题.  
做过的项目中,上传都是直传oss,后端如何处理都不甚了了.趁此机会来接触下.  

### 逻辑梳理
不同于小文件的一锤子买卖,大文件上传,需要将文件分片切割后上传,步骤比小文件更多.
站在前端的角度,可分为3个阶段
1. 查询后端是否存在未完成的上传任务.
2. 上传分片
3. 通知后端合并分片  

后端在第一步中,需要做一些判断和操作.  
如果存在未完成的任务,将已完成的分片标识通知前端.  
如果没有,则创建一个任务,通知前端可以开始上传.  

### 代码编写
#### 起项目 
首先创建项目.express+vue 
1. `npm i -g express-generator`,用来生成express项目.
2. `express data-spliter`.
3. `cd data-spliter && vue create web_site`创建前端项目

#### 编写前端
使用vue只是为了搭建一个ES6脚手架,框架不是重点,不依赖vue.  
重点是input file载入一个文件后的处理逻辑.  
为了方便处理,创建一个`FileUploader`类来处理上传逻辑.  
```javascript
class FileUploader{
  state = {
    md5: "",        // 本次上传的文件md5
    chunkList: [],  // 文件分片
    chunkLength: 0, // 文件分片数量
    fileSize: 0,    // 文件大小
    taskMap: [],    // 任务容器
  };

  constructor(file, opt = {}) {
    this.$file = file;
    this.state.fileSize = file.size;
    this.$options = { ...dftOptions, ...opt };
  }
  async upload() {
  }
  async check() {}
}

```
按前述逻辑,第一步是创建任务,或者检查是否存在未完成的任务.这里就需要一个标识,可以来标记一个文件.这个可以引入md5来处理.比如[spark-md5](https://www.npmjs.com/package/spark-md5).  
正好它的文档中提供了处理大文件的示例代码.c+v大法走起.  
稍微改动下.如下所示:
```javascript
class FileUploader{
 fileMd5(file) {
    return new Promise((resolve) => {
      const {
        $options: { chunkSize },
        state: { chunkList },
      } = this;
      const chunks = Math.ceil(file.size / chunkSize);
      let currentChunk = 0;
      const spark = new SparkMd5.ArrayBuffer();
      const fileReader = new FileReader();

      fileReader.onload = (e) => {
        console.log("read chunk nr", currentChunk + 1, "of", chunks);
        spark.append(e.target.result);
        chunkList.push(e.target.result);
        currentChunk++;

        if (currentChunk >= chunks)
          return resolve({ success: true, data: spark.end() });
        loadNext();
      };

      fileReader.onerror = () => {
        resolve({ success: false, message: "oops, something went wrong." });
      };

      function loadNext() {
        var start = currentChunk * chunkSize,
          end = start + chunkSize >= file.size ? file.size : start + chunkSize;

        fileReader.readAsArrayBuffer(blobSlice.call(file, start, end));
      }

      loadNext();
    });
  }
}
```
在分片将文件喂给sparkMd5的同时,我们将分片存到实例上.
有了唯一标识,就可以去服务端查询了.
```javascript
class FileUploader{
  async check() {
     const md5 = await this.fileMd5(this.$file);
     if (!md5.success) return this.error("md5 解析失败!");
     this.state.md5 = md5.data;
     this.state.chunkLength = this.state.chunkList.length;
     // 每个分片建立一个任务 
     this.state.taskMap = [...Array(this.state.chunkLength)].reduce(
       (p, n, i) => [
         ...p,
         {
           id: i,
           loaded: false,
           progress: 0,
         },
       ],
       []
     );
 
     return API.check({ md5: md5.data });
  }
}
```
服务端应使用这个标识,查询是否存在未完成任务,如果存在,返回已完成的分片id.
前端则将对应的id置为完成状态,避免重复上传,也就是断点续传.
```javascript
class FileUploader{
  async upload() {
    const check = await this.check();
    if (!check.success) return check;
    check.data.forEach((id) => {
      const task = this.state.taskMap.find((it) => it.id === id);
      task.loaded = true;
      task.progress = 1;
    });
    const upload = await this.doUpload();
  }
}
```
上一步骤,创建并初始化好了一个任务表,接下来就是将这些任务表依次完成
> 浏览器同一个域名最多6个请求,多了排队,所以直接同时发出也没有问题.
```javascript
class FileUploader{
  async doUpload() {
    const result = await Promise.all(
      this.state.taskMap.map((task) => this.uploadChunk(task))
    );
    if (result.some((it) => !it.success))
      return this.error("上传失败,请稍后重试!");
    return { success: true, message: "所有分块上传成功!" };
  }

  async uploadChunk(task) {
    if (task.loaded) return Promise.resolve({ success: true });

    const data = {
      // 取出之前分片的base64字符串,重新转为Blob
      file: new Blob([this.state.chunkList[task.id]]),
      total: this.state.chunkLength,
      md5: this.state.md5,
      id: task.id,
    };

    const onProgress = (e) => {
      task.progress = e.loaded / e.total;
      // 通知实例当前任务进度
      this.fire("progress", this.state.taskMap);
    };

    return API.uploadChunk(data, onProgress);
  }

}
```
所有分片上传成功,即可通知服务端合并分片.该接口完成后,应返回上传文件的路径.

#### 编写后端
添加一个`routes/api.js`专门处理上传请求.
在app.js中引入并use.这个就不附代码了.
`api.js`非常简单,就是分发任务
```javascript
var express = require("express");
var router = express.Router();
const multer = require("multer");
const file = require("./file");

const upload = multer();

router.post("/check", async function (req, res) {
  res.send(await file.getChildren(req.body.md5));
});

router.post("/uploadChunk", upload.single("file"), async function (req, res) {
  res.send(await file.saveChunk(req.file.buffer, req.body));
});

router.post("/complete", async function (req, res) {
  res.send(await file.complete(req.body));
});

module.exports = router;
```
主要的逻辑在`routes/file.js`
```javascript
const path = require("path");
const fs = require("fs");
const temp = (p) => path.resolve(__dirname, "../data/temp/", p);

const sort = (list) => list.sort((a, b) => a - b);

function createDir(name) {
  return new Promise((resolve) => {
    fs.mkdir(temp(name), (err) => {
      if (err) {
        return resolve({ success: false, message: "创建失败", err });
      }
      return resolve({
        success: true,
        message: "临时文件夹创建成功!",
        data: [],
      });
    });
  });
}

function getChildren(name) {
  return new Promise((resolve) => {
    fs.readdir(temp(name), async (err, file) => {
      if (err) {
        return resolve(createDir(name));
      }
      return resolve({
        success: true,
        message: "获取上传进度成功!",
        data: sort(file),
      });
    });
  });
}

function saveChunk(file, state) {
  return new Promise((resolve) => {
    const name = temp(state["md5"] + "/" + state["id"]);
    const chunkName = "分块" + state["id"];
    fs.writeFile(name, file, (err) => {
      if (err)
        return resolve({
          success: false,
          message: chunkName + "保存失败",
          err,
        });
      resolve({ success: true, message: chunkName + "保存成功" });
    });
  });
}

function complete(state) {
  return new Promise((resolve) => {
    const dir = temp(state.md5);
    fs.access(dir, fs.constants.F_OK, async (err) => {
      if (err) {
        return resolve({ success: false, message: "文件夹不存在!" });
      }
      const result = await mergeFile(state);
      if (!result.success) return result;
      fs.rmdir(dir, (err) => {
        if (err) return resolve({ success: false, message: "合并失败!", err });
        resolve({
          success: true,
          message: "上传成功!",
          data: {
            path: "/resources/" + state.name,
          },
        });
      });
    });
  });
}

function mergeFile(state) {
  return new Promise((resolve) => {
    const dir = temp(state.md5);
    const out = fs.createWriteStream(
      path.resolve(__dirname, "../public/resources/" + state.name)
    );
    fs.readdir(dir, async (err, file) => {
      file = sort(file);
      if (err) {
        resolve({ success: false, err, message: "合并失败!" });
        return;
      }
      const rsError = () => resolve({ success: false, message: "合并失败" });

      while (file.length) {
        const id = file.shift();
        const chunkPath = temp(state.md5 + "/" + id);
        const rs = fs.createReadStream(chunkPath);
        let success = await writeChunk(rs, out);
        if (!success) {
          rsError();
          break;
        }
        success = await delFile(chunkPath);
        if (!success) {
          rsError();
          break;
        }
      }

      out.end();

      resolve({ success: true, message: "合并成功!" });
    });
  });
}

function writeChunk(rs, ws) {
  return new Promise((resolve) => {
    rs.on("data", (chunk) => ws.write(chunk));
    rs.on("end", () => resolve(true));
    rs.on("error", () => resolve(false));
  });
}

function delFile(path) {
  return new Promise((resolve) => {
    fs.unlink(path, (e) => resolve(!e));
  });
}

module.exports = {
  getChildren,
  saveChunk,
  complete,
};
```
这个部分也不复杂,就是对fs的一些基本操作.  
值得说说的是合并分片的逻辑.  
服务端资源有限,要避免一次性加载所有文件进行读写这种操作,合并文件这种逻辑,应该使用流的方式.  
首先创建一个可写流`out`  
```javascript
  const out = fs.createWriteStream(
      path.resolve(__dirname, "../public/resources/" + state.name)
    );
```
然后遍历分片,对于每个分片,创建一个可读流,然后将可读流写进`out`可写流中,这样就可以细水长流,避免一次性加载所有分片,导致占用内存.  
没写完一个分片,就将其删除.最后合成文件后,将文件目录也同时删除.  

### 总结
时隔良久,很多细节想不起来了,结果又是虎头蛇尾的一篇,拖延症要不得啊/(ㄒoㄒ)/~~  

